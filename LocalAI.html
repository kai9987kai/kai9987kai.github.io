<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Enhanced AI Interface (Debug Logging)</title>
  <style>
    #app {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-family: Arial, sans-serif;
    }
    h1, h2 { color: #333; text-align: center; }
    .section { margin-bottom: 20px; }
    textarea, input, select {
      width: 100%;
      margin-bottom: 10px;
      padding: 8px;
      box-sizing: border-box;
    }
    button {
      padding: 10px 15px;
      margin-right: 10px;
      margin-bottom: 10px;
      background-color: #007BFF;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
    }
    button:hover { background-color: #0056b3; }
    #response, #predict-output, #logs {
      background-color: #f0f0f0;
      padding: 10px;
      min-height: 50px;
      border-radius: 5px;
    }
    canvas { margin-top: 10px; }
    #progress-container {
      width: 100%;
      background-color: #eee;
      border-radius: 5px;
      overflow: hidden;
      margin-bottom: 10px;
      height: 25px;
    }
    #progress-bar {
      height: 100%;
      width: 0%;
      background-color: #007BFF;
      transition: width 0.3s;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/papaparse@5.3.2/papaparse.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
<div id="app">
  <h1>Enhanced AI Interface (Debug Logging)</h1>

  <!-- Language Model Section -->
  <div id="language-model-section" class="section">
    <h2>Language Model</h2>
    <textarea id="text-input" placeholder="Enter text..."></textarea>
    <input type="file" id="image-input" accept="image/*">
    <div>
      <button id="pretrain-btn">Pre-train</button>
      <button id="finetune-btn">Fine-tune</button>
      <button id="generate-btn">Generate Response</button>
      <button id="save-language-btn">Save Language Model</button>
      <button id="load-language-btn">Load Language Model</button>
    </div>
    <div id="response"></div>
  </div>

  <!-- Tabular Data Training Section -->
  <div id="tabular-section" class="section">
    <h2>Tabular Data Training</h2>
    <input type="file" id="csv-input" accept=".csv">
    <select id="task-type">
      <option value="classification">Classification</option>
      <option value="regression">Regression</option>
    </select>
    <input type="number" id="epochs" value="10" min="1" placeholder="Epochs">
    <input type="number" id="batch-size" value="32" min="1" placeholder="Batch Size">
    <input type="number" id="learning-rate" value="0.001" step="0.0001" placeholder="Learning Rate">
    <input type="number" id="val-split" value="0.2" step="0.1" min="0" max="1" placeholder="Validation Split">
    <div>
      <button id="train-tabular-btn">Train Tabular Model</button>
      <button id="predict-tabular-btn">Predict</button>
      <button id="save-tabular-btn">Save Tabular Model</button>
      <button id="load-tabular-btn">Load Tabular Model</button>
    </div>
    <input type="text" id="predict-input" placeholder="Enter features (comma-separated)">
    <div id="predict-output"></div>
    <canvas id="loss-chart" width="400" height="200"></canvas>
    <div id="progress-container">
      <div id="progress-bar"></div>
    </div>
  </div>

  <!-- Logs Section -->
  <div id="logs-section" class="section">
    <h2>Logs</h2>
    <div id="logs"></div>
  </div>
</div>

<script>
  // Global references
  if (typeof window.languageModel === "undefined") window.languageModel = null;
  if (typeof window.tabularModel === "undefined") window.tabularModel = null;

  window.appendLog = function(msg) {
    const logContainer = document.getElementById("logs");
    const t = new Date().toLocaleTimeString();
    logContainer.innerHTML += "[" + t + "] " + msg + "<br>";
    logContainer.scrollTop = logContainer.scrollHeight;
  };

  (async function(){
    await tf.ready();

    // ========== Language Model with MoE ==========
    class MoELayer extends tf.layers.Layer {
      static className = 'MoELayer';
      constructor(numExperts, inputDim, outputDim, config) {
        super(config);
        this.numExperts = numExperts;
        this.inputDim = inputDim;
        this.outputDim = outputDim;
      }
      build(inputShape) {
        this.experts = [];
        for (let i = 0; i < this.numExperts; i++) {
          this.experts.push(this.addWeight(`expert_${i}`, [this.inputDim, this.outputDim], 'float32', tf.initializers.glorotNormal()));
        }
        this.gate = this.addWeight('gate', [this.inputDim, this.numExperts], 'float32', tf.initializers.glorotNormal());
        super.build(inputShape);
      }
      call(inputs) {
        const gateScores = tf.matMul(inputs, this.gate.read());
        const gateProbs = tf.softmax(gateScores, -1);
        const expertOutputs = this.experts.map(expert => tf.matMul(inputs, expert.read()));
        const stackedExperts = tf.stack(expertOutputs, 1);
        const weightedOutputs = tf.mul(stackedExperts, gateProbs.expandDims(2));
        return tf.sum(weightedOutputs, 1);
      }
      getConfig() {
        const baseConfig = super.getConfig();
        return Object.assign(baseConfig, {
          numExperts: this.numExperts,
          inputDim: this.inputDim,
          outputDim: this.outputDim
        });
      }
    }
    tf.serialization.registerClass(MoELayer);

    const maxTextLength = 10;
    const embeddingDim = 16;
    const imageFeatureDim = 1280;
    const combinedDim = 128;
    const vocabSize = 100;
    const numExperts = 4;
    const vocab = Array.from({ length: vocabSize }, (_, i) => `word${i}`);

    function tokenize(text) {
      const words = text.toLowerCase().split(/\s+/);
      return words.map(word => vocab.indexOf(word) !== -1 ? vocab.indexOf(word) : 0).slice(0, maxTextLength);
    }

    function padSequence(seq) {
      const padded = new Array(maxTextLength).fill(0);
      seq.forEach((val, idx) => { if (idx < maxTextLength) padded[idx] = val; });
      return padded;
    }

    async function processImage(file) {
      const imgData = await new Promise(resolve => {
        const reader = new FileReader();
        reader.onload = e => resolve(e.target.result);
        reader.readAsDataURL(file);
      });
      const img = document.createElement('img');
      img.src = imgData;
      await new Promise(resolve => { img.onload = resolve; });
      const tensor = tf.browser.fromPixels(img).resizeNearestNeighbor([224, 224]).toFloat().expandDims().div(255.0);
      const mobilenet = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v2_1.0_224/model.json');
      return mobilenet.predict(tensor).flatten();
    }

    function buildLanguageModel() {
      const textInput = tf.input({ shape: [maxTextLength] });
      const imageInput = tf.input({ shape: [imageFeatureDim] });
      const embeddingLayer = tf.layers.embedding({ inputDim: vocabSize, outputDim: embeddingDim });
      const textEmbedding = embeddingLayer.apply(textInput);
      const flattenedText = tf.layers.flatten().apply(textEmbedding);
      const textProj = tf.layers.dense({ units: combinedDim }).apply(flattenedText);
      const imageProj = tf.layers.dense({ units: combinedDim }).apply(imageInput);
      const combined = tf.layers.concatenate().apply([textProj, imageProj]);
      const fused = tf.layers.dense({ units: combinedDim, activation: 'relu' }).apply(combined);
      const moeOutput = new MoELayer(numExperts, combinedDim, combinedDim).apply(fused);
      const logits = tf.layers.dense({ units: vocabSize, activation: 'softmax' }).apply(moeOutput);
      window.languageModel = tf.model({ inputs: [textInput, imageInput], outputs: logits });
      window.languageModel.compile({ optimizer: 'adam', loss: 'sparseCategoricalCrossentropy' });
    }
    buildLanguageModel();

    async function pretrainLanguageModel() {
      const imageFile = document.getElementById('image-input').files[0];
      if (!imageFile) { alert("Please upload an image."); return; }
      const tokenized = padSequence(tokenize("hello world"));
      const textTensor = tf.tensor2d([tokenized]);
      const imageTensor = (await processImage(imageFile)).reshape([1, imageFeatureDim]);
      const target = tf.tensor1d([0], 'int32');
      await window.languageModel.fit([textTensor, imageTensor], target, { epochs: 1 });
      document.getElementById('response').innerText = "Pre-training completed.";
    }

    async function finetuneLanguageModel() {
      const text = document.getElementById('text-input').value;
      const imageFile = document.getElementById('image-input').files[0];
      if (!text || !imageFile) { alert("Please provide both text and image for fine-tuning."); return; }
      const tokenized = padSequence(tokenize(text));
      const textTensor = tf.tensor2d([tokenized]);
      const imageTensor = (await processImage(imageFile)).reshape([1, imageFeatureDim]);
      const target = tf.tensor1d([0], 'int32');
      await window.languageModel.fit([textTensor, imageTensor], target, { epochs: 1 });
      document.getElementById('response').innerText = "Fine-tuning completed.";
    }

    async function generateResponse() {
      const text = document.getElementById('text-input').value;
      const imageFile = document.getElementById('image-input').files[0];
      if (!text || !imageFile) { alert("Please provide both text and image inputs."); return; }
      const tokenized = padSequence(tokenize(text));
      const textTensor = tf.tensor2d([tokenized]);
      const imageTensor = (await processImage(imageFile)).reshape([1, imageFeatureDim]);
      const prediction = window.languageModel.predict([textTensor, imageTensor]);
      const classIdx = tf.argMax(prediction, 1).dataSync()[0];
      document.getElementById('response').innerText = `Predicted Class: ${classIdx}`;
    }

    async function saveLanguageModel() {
      await window.languageModel.save('localstorage://language-model');
      appendLog("Language model saved.");
    }

    async function loadLanguageModel() {
      try {
        window.languageModel = await tf.loadLayersModel('localstorage://language-model');
        appendLog("Language model loaded.");
      } catch (error) {
        appendLog("Error loading language model: " + error);
      }
    }

    document.getElementById('pretrain-btn').addEventListener('click', pretrainLanguageModel);
    document.getElementById('finetune-btn').addEventListener('click', finetuneLanguageModel);
    document.getElementById('generate-btn').addEventListener('click', generateResponse);
    document.getElementById('save-language-btn').addEventListener('click', saveLanguageModel);
    document.getElementById('load-language-btn').addEventListener('click', loadLanguageModel);

    // ---------------- Tabular Data Section ----------------
    if (typeof window.minFeatures === "undefined") window.minFeatures = null;
    if (typeof window.maxFeatures === "undefined") window.maxFeatures = null;
    if (typeof window.numericFeatureCount === "undefined") window.numericFeatureCount = 0;

    const lossChartCtx = document.getElementById('loss-chart').getContext('2d');
    const tabLossChart = new Chart(lossChartCtx, {
      type: 'line',
      data: {
        labels: [],
        datasets: [
          { label: 'Training Loss', data: [], borderColor: 'blue', fill: false },
          { label: 'Validation Loss', data: [], borderColor: 'red', fill: false }
        ]
      },
      options: { scales: { y: { beginAtZero: true } } }
    });

    async function parseCSV(file) {
      return new Promise((resolve, reject) => {
        Papa.parse(file, {
          header: true,
          dynamicTyping: true,
          skipEmptyLines: true,
          complete: (results) => {
            if (!results.data || !results.data.length) {
              reject("CSV file is empty or invalid.");
            } else {
              resolve(results.data);
            }
          },
          error: (err) => {
            reject(err);
          }
        });
      });
    }

    function detectColumnTypes(rows, columns) {
      const colTypes = {};
      columns.forEach(col => colTypes[col] = "numeric");
      rows.forEach((row, idx) => {
        columns.forEach(col => {
          const val = row[col];
          if (val === undefined || val === null) return;
          if (typeof val !== "number" || isNaN(val)) {
            colTypes[col] = "string";
          }
        });
      });
      return colTypes;
    }

    function oneHotEncodeColumn(rows, col) {
      const uniqueVals = Array.from(new Set(rows.map(r => r[col]).filter(v => v!==undefined && v!==null)));
      const map = {};
      uniqueVals.forEach((val, idx) => map[val] = idx);
      return { uniqueVals, map };
    }

    function buildFeatureMatrix(rows, columns, colTypes, labelCol, taskType) {
      const encoders = {};
      columns.forEach(col => {
        if (colTypes[col] === "string") {
          encoders[col] = oneHotEncodeColumn(rows, col);
        }
      });
      let labelMap = null;
      if (taskType === "classification") {
        const uniqueLabels = Array.from(new Set(rows.map(r => r[labelCol]).filter(v => v!==undefined && v!==null)));
        labelMap = {};
        uniqueLabels.forEach((l, i) => { labelMap[l.toString()] = i; });
      }
      const finalFeatures = [];
      const finalLabels = [];
      let skipCount = 0;
      for (let i=0; i<rows.length; i++) {
        const row = rows[i];
        if (row[labelCol] === undefined || row[labelCol] === null) {
          appendLog(`Skipping row ${i} - missing label`);
          skipCount++;
          continue;
        }
        let featureVector = [];
        let skipRow = false;
        columns.forEach(col => {
          const val = row[col];
          if (colTypes[col] === "numeric") {
            if (typeof val !== "number" || isNaN(val)) {
              appendLog(`Skipping row ${i} - numeric parse fail on col "${col}"`);
              skipRow = true;
            } else {
              featureVector.push(val);
            }
          } else {
            // one-hot
            const idx = encoders[col].map[val];
            const arr = new Array(encoders[col].uniqueVals.length).fill(0);
            if (idx !== undefined) arr[idx] = 1;
            featureVector = featureVector.concat(arr);
          }
        });
        if (skipRow) { skipCount++; continue; }
        let labelVal;
        if (taskType === "classification") {
          labelVal = labelMap[row[labelCol].toString()];
          if (labelVal === undefined) {
            appendLog(`Skipping row ${i} - unknown label value "${row[labelCol]}"`);
            skipCount++;
            continue;
          }
        } else {
          let numericVal = Number(row[labelCol]);
          if (isNaN(numericVal)) {
            appendLog(`Skipping row ${i} - label parse fail on regression`);
            skipCount++;
            continue;
          }
          labelVal = numericVal;
        }
        finalFeatures.push(featureVector);
        finalLabels.push(labelVal);
      }
      appendLog(`Skipped ${skipCount} rows. Using ${finalFeatures.length} valid rows.`);
      return { finalFeatures, finalLabels, encoders, labelMap };
    }

    function createTabularModel(inputDim, taskType, labelCount) {
      const m = tf.sequential();
      m.add(tf.layers.dense({ units: 64, activation: 'relu', inputShape: [inputDim] }));
      m.add(tf.layers.dense({ units: 32, activation: 'relu' }));
      if (taskType === 'classification') {
        m.add(tf.layers.dense({ units: labelCount, activation: 'softmax' }));
        m.compile({ optimizer: 'adam', loss: 'sparseCategoricalCrossentropy', metrics: ['accuracy'] });
      } else {
        m.add(tf.layers.dense({ units: 1 }));
        m.compile({ optimizer: 'adam', loss: 'meanSquaredError' });
      }
      return m;
    }

    window.minFeatures = null;
    window.maxFeatures = null;
    window.numericFeatureCount = 0;

    async function trainTabularModel() {
      const file = document.getElementById('csv-input').files[0];
      if (!file) { alert("Please upload a CSV file"); return; }
      try {
        const rows = await parseCSV(file);
        if (!rows.length) {
          appendLog("No data in CSV");
          return;
        }
        const columns = Object.keys(rows[0]);
        if (columns.length < 2) {
          appendLog("CSV must have at least 2 columns (features + label).");
          return;
        }
        const labelCol = columns[columns.length - 1];
        const featureCols = columns.slice(0, -1);
        const taskType = document.getElementById('task-type').value;
        const colTypes = detectColumnTypes(rows, featureCols);
        const { finalFeatures, finalLabels, encoders, labelMap } = buildFeatureMatrix(rows, featureCols, colTypes, labelCol, taskType);
        if (!finalFeatures.length) {
          appendLog("No valid data in CSV after processing. All rows invalid?");
          return;
        }
        const featureTensor = tf.tensor2d(finalFeatures);
        window.minFeatures = featureTensor.min(0);
        window.maxFeatures = featureTensor.max(0);
        const normalizedFeatures = featureTensor.sub(window.minFeatures).div(window.maxFeatures.sub(window.minFeatures).add(1e-8));
        let labelTensor;
        if (taskType === "classification") {
          labelTensor = tf.tensor1d(finalLabels, 'int32');
        } else {
          labelTensor = tf.tensor1d(finalLabels);
        }
        window.numericFeatureCount = normalizedFeatures.shape[1];
        const labelCount = (taskType === 'classification') ? (new Set(finalLabels)).size : null;
        window.tabularModel = createTabularModel(window.numericFeatureCount, taskType, labelCount);
        const epochs = parseInt(document.getElementById('epochs').value);
        const batchSize = parseInt(document.getElementById('batch-size').value);
        const learningRate = parseFloat(document.getElementById('learning-rate').value);
        const valSplit = parseFloat(document.getElementById('val-split').value);
        await window.tabularModel.fit(normalizedFeatures, labelTensor, {
          epochs,
          batchSize,
          validationSplit: valSplit,
          callbacks: {
            onEpochEnd: (epoch, logs) => {
              appendLog(`Epoch ${epoch + 1}: Loss=${logs.loss.toFixed(4)}, ValLoss=${logs.val_loss.toFixed(4)}`);
            }
          }
        });
        appendLog("Tabular model training completed.");
      } catch (err) {
        appendLog(`Error: ${err}`);
      }
    }

    async function predictTabular() {
      if (!window.tabularModel) { alert("Please train or load a model first"); return; }
      const inputStr = document.getElementById('predict-input').value.trim();
      if (!inputStr) return;
      const parts = inputStr.split(',').map(x => parseFloat(x));
      if (parts.length !== window.numericFeatureCount) {
        alert("Input features do not match the final model's feature dimension");
        return;
      }
      if (parts.some(x => isNaN(x))) {
        alert("Some input feature is not numeric in Predict. This script doesn't handle that.");
        return;
      }
      const inputTensor = tf.tensor2d([parts]);
      const normed = inputTensor.sub(window.minFeatures).div(window.maxFeatures.sub(window.minFeatures).add(1e-8));
      const prediction = window.tabularModel.predict(normed);
      const taskType = document.getElementById('task-type').value;
      let output;
      if (taskType === 'classification') {
        const classIndex = prediction.argMax(-1).dataSync()[0];
        output = `Predicted class: ${classIndex}`;
      } else {
        const value = prediction.dataSync()[0];
        output = `Predicted value: ${value.toFixed(4)}`;
      }
      document.getElementById('predict-output').innerText = output;
    }

    async function saveTabularModel() {
      if (!window.tabularModel) { alert("No model to save"); return; }
      await window.tabularModel.save('localstorage://tabular-model');
      appendLog("Tabular model saved.");
    }

    async function loadTabularModel() {
      try {
        window.tabularModel = await tf.loadLayersModel('localstorage://tabular-model');
        appendLog("Tabular model loaded.");
      } catch (error) {
        appendLog("Error loading model: " + error);
      }
    }

    document.getElementById('train-tabular-btn').addEventListener('click', trainTabularModel);
    document.getElementById('predict-tabular-btn').addEventListener('click', predictTabular);
    document.getElementById('save-tabular-btn').addEventListener('click', saveTabularModel);
    document.getElementById('load-tabular-btn').addEventListener('click', loadTabularModel);

  })();
</script>
</body>
</html>
